# Standard_Tools
A subset of the tools that are a part of the Standardize_Metadata 
module that can be used for analysis of csv files generated by 
metadata.py <br />

## Overview of Current Tools

**The following tools all attempt to curate and parse an input
string into a standardized format. The nuances of each module are
highlighted below** <br />

### Pass_Through_Terms/pass_through_terms.py
Passes values through without parsing them. Returns: [the input value].
Current values include: <br />
1. Primary Run ID (RUN/PRIMARY_ID) <br />
2. Primary Sample ID (SAMPLE/PRIMARY_ID) <br />
3. Experiment ID (EXPERIMENT_REF/PRIMARY_ID) <br />
4. Study ID (STUDY/PRIMARY_ID) <br />
5. size <br />
6. total number of bases (total_bases) <br />
7. spot length (SPOT_LENGTH) <br />
8. total number of spots (total_spots) <br />
9. organism name (organism) <br />
10. platform (PLATFORM) <br />
11. instrument model (INSTRUMENT_MODEL) <br />
12. sub species (sub_species, Subspecies) <br />

### Collection_Date/collection_date.py
Curates and parses dates. Returns: <br />
1. The curated date (if it exists) <br />
2. An error value <br />
3. The original date if it fails to parse <br />
4. An ambiguity flag <br />

### Geographic_Location/geographic_location.py
Curates and parses various geographic locations. Returns: <br />
1. The curated location (if it exists) <br />
2. The original date if it fails to parse <br />
3. Additional information regarding unparsed values <br />
This module uses various standardized lists in its directory to
standardize provinces, countries, and acronyms. <br />

### Serovar/serovar.py
Curates and parses serovars. Returns: <br />
1. The curated serovar, the serovar's variant <br />
2. Other serovar info from the standard list <br />
3. The original serovar <br />
4. A spellchecked suggestion. <br />
This module uses standardized lists in its
directory and suggestions are generated by the spellcheck module. <br />

### Isolation_Source/isolation_source.py
Curates and parses isolation sources. Returns: <br />
1. The curated isolation source <br />
2. The original serovar <br />
3. A spellchecked suggestion. <br />
This module uses standardized lists in its directory and suggestions are
generated by the spellcheck module. <br />

### Organization_Name/organization_name.py
Curates and parses organization names. Returns: <br />
1. The curated organization name <br />
2. The original organization name <br />
3. A spellchecked suggestion <br />
4. The standardized country <br />
5. The standardized province. <br />
This module uses the standardized lists in its directory and suggestions
are generated by the spellcheck module. <br />

### Host/host.py
Like Isolation Source and Organization Name (see above) <br />

### ./post_processing.py
Special scripts to make more out of the data. These run after all
of the standardization scripts have run. Current features include: <br />
1. Inference of standard dates from ambiguous dates based on location <br />
2. Location suggestions based on organization names <br />

## Interpreting the results

### Collection Date
Collection Date will attempt to parse dates into the following curated format:
YYYY-MMM-DD where the month is the three letter acronym (ex. Jan). If the date
is inexact (ex. 2007), the script will return the halfway date and a value of
error. In the case of 2007, it would return 2007-Jun-01, error 182, meaning the
date could fall within 182 days before or after that date. The ambiguous value
indicates whether the month and day values. <br />

### Geographic Location
Geographic Location will attempt to parse a geographic location, taking into
account country as well as province. The curated format consists of the
curated country value followed by a semicolon, followed by the curated province
value (ex. Canada:Ontario). If a curated province does not exist, only the
curated country will be provided. Collection notes will return any non-curated
values, which can consist of country only (Wonderland), country and province
(Wonderland:Queen's Castle), or province only (:Queen's Castle). <br />

### Serovar
Serovar will attempt to parse a serovar into three columns of information:
the curated serovar, the curated variant, and the curated other info. Failing
to do so, the script will attempt to spellcheck the serovar to a standard
one. <br />

### Isolation Source
Isolation Source will attempt to standardize isolation sources of the same type
into a common term. If the term is not found in the standard list, it will
attempt to spellcheck it. <br />

### Organization Name
Organization Name will attempt to standardize organization names. If a term
is not found in the standard list, it will attempt to spellcheck it. <br />

### Host Name
Like Isolation Source and Organization Name (see above). <br />

### Post Processing
See individual Post Processing functions for documentation. The module
will put all results in columns labeled putative_* (where * is
descriptive of the given field). These columns will be appended to
the standardized fields. <br />

## How to add a New Standardization Module
1. Make a new directory in Analysis_Tools <br />
2. Add your parsing script (written in python) to this directory
   * Make sure your parsing script has the following: <br />
     a) column_strs: a list of headers which you wish to retrieve for parsing
                     with this module <br />
     b) keys: a list of items for which to reference for return values.
              This list must be the same length as the list returned by the
              parse function (see below) <br />
     c) parse(args): a function which parses input data and outputs a list of
                     terms (each term corresponds to an element in keys) <br />
3. Add your new module to the modules variable in standardize.py as a
   two-element list, where the first term is the folder name of the module, and
   the second term is the python script name (without the extension). <br />
4. If your module does not require additional files to parse, add it to
   the variable simple_parse which acts as a sort of exception list for
   additional data. <br />
Side note: If your parser uses additional files, reference any one of the
existing modules (ex. geographic location) for how to implement these. It
involves some special consideration with regards to file structure and imports. <br />

## Adding a new Post Processing Function
1. Write your function in post_processing.py such that it mutates each
line of the data passing through. <br />
2. Append the new header name directly afterwards. <br />
* Note: This is not the most elegant solution for this problem and
and should probably be cleaned up if it begins to get bloated. If only
adding a couple fields however, feel free to do it as mentioned above
and to use the current functions as templates. <br />

## Building/Updating new standard lists and spellchecker files:
1. Set up your standard lists as having 1 column for your queries
   and >=1 column for your standard terms. <br />
2. Take the standard terms column (the most important one, if more than one)
   and make it into its own standard terms list. <br />
3. Use ./SpellCheck/BuildSearchIndex.py to create an index file from
   the standard terms list in step 2. <br />